{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Classifiers\n",
    "*Paulo G. Martinez* Sun. Apr. 5, 2020\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classifier-kitchen-sink\n",
    "Attempt efficient classifier comparison and promotion\n",
    "\n",
    "## This is an experiment in \"elegant brute force.\"\n",
    "- Given a \"large\" but \"tidy\" data set with a \"wide\" set of potentially sparse and or redundant numeric, categorical, and datetime features and multiple \"imbalaced\" targets (not a multi class column, but two or more binary class target columns)\n",
    "  - Attempt to find an efficient way of comparing the success of various classification models \n",
    "  -   each with various model configurations\n",
    "\n",
    "P1. One of the premises of this experiment is that it would take \"too much\" time to assess the viability, let alone solve the problem using a \"traditional\" domain-knowledge based approach.\n",
    "\n",
    "P2. Another premise is that compute and memory resources are so limited that a brute force iteration through models would also take \"too long.\"\n",
    "\n",
    "## Workflow 1.1\n",
    "\"Failing Fast Promote Fast\" We begin with the most challenging but easiest to compute configurations in hopes of fiding success before having to \"bloat\" all the way up to full brute-force iteration.\n",
    "\n",
    "### We define the \"rote\" order of brute force iteration of tasks to be tested\n",
    "**small samples**\n",
    "- natural to balanced weights\n",
    "['small-sample-natural-weigt-model1', 'small-sample-natural-weight-model2', ..., 'small-sample-natural-weight-modeln'] + \n",
    "['small-sample-mild-reweight-model1', 'small-sample-mild-reweight-model2', ..., 'small-sample-mild-reweight-modeln'] + \n",
    "...\n",
    "['small-sample-aggressive-reweight-model1', 'small-sample-aggressive-reweight-model2', ..., 'small-sample-aggressive-reweight-modeln']\n",
    "\n",
    "**medium samples**\n",
    "- ibid, mutatis mutandi\n",
    "\n",
    "**large samples**\n",
    "- ibid, mutatis mutandi\n",
    "\n",
    "**full data**\n",
    "- ibid, mutatis mutandi\n",
    "\n",
    "### we iterate through the un-tested tasks\n",
    "- We begin at the smallest untested task\n",
    "    - but at the most difficult untested version of the task (class imbalance)\n",
    "        - we may manipulate balance of training data, but not of test data!\n",
    "        - we iterate through models to survey performance\n",
    "            - each model does a small number of feature selection optimizations\n",
    "        - **at this point we have a \"bearish\" approach, wanting to preview models before committing compute resources to them**\n",
    "            - recording which configurations we have already tried\n",
    "            - **if a model succeeds at performing satisfactorily, we switch to an inreasingly bullish approach assuming this model will generalize well**\n",
    "                - we start a new recursion of this same pattern but on only that model's configurations\n",
    "                    - but for every success we double the number of intermediate tasks to skip\n",
    "                        - ex. if tasks were [smallest, smaller, small, medium, large, larger, largest] and model succedes at smallest\n",
    "                            - we skip smaller and go strainng to small\n",
    "                                - if model succeeds a second time, we skip to larger\n",
    "                - recording which connfigurations we have already tried\n",
    "                - if model fails \n",
    "                    - we exit this recursion and return to the original pattern\n",
    "                    \n",
    "**Note: This would only be more efficient if we hypothesize that a model will generalize well to unseen data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reveal data structures for clarity during dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>EMBARKED</th>\n",
       "      <th>EMBARKED_S</th>\n",
       "      <th>EMBARKED_C</th>\n",
       "      <th>EMBARKED_Q</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>braund</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>cumings</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>pc</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>heikkinen</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>stono</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>futrelle</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>allen</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass       Name     Sex   Age  SibSp  Parch Ticket     Fare  \\\n",
       "0       0.0       3     braund    male  22.0      1      0      a   7.2500   \n",
       "1       1.0       1    cumings  female  38.0      1      0     pc  71.2833   \n",
       "2       1.0       3  heikkinen  female  26.0      0      0  stono   7.9250   \n",
       "3       1.0       1   futrelle  female  35.0      1      0    NaN  53.1000   \n",
       "4       0.0       3      allen    male  35.0      0      0    NaN   8.0500   \n",
       "\n",
       "  Cabin EMBARKED  EMBARKED_S  EMBARKED_C  EMBARKED_Q  \n",
       "0   NaN        S        True       False       False  \n",
       "1   C85        C       False        True       False  \n",
       "2   NaN        S        True       False       False  \n",
       "3  C123        S        True       False       False  \n",
       "4   NaN        S        True       False       False  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the data we'll be developing on, it's a slightly modified version of the titanic data set\n",
    "if 'pd' not in vars():\n",
    "    import pandas as pd\n",
    "pd.read_csv('data/semi_processed_all.csv').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1309 entries, 0 to 1308\n",
      "Data columns (total 14 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Survived    891 non-null    float64\n",
      " 1   Pclass      1309 non-null   int64  \n",
      " 2   Name        1309 non-null   object \n",
      " 3   Sex         1309 non-null   object \n",
      " 4   Age         1046 non-null   float64\n",
      " 5   SibSp       1309 non-null   int64  \n",
      " 6   Parch       1309 non-null   int64  \n",
      " 7   Ticket      352 non-null    object \n",
      " 8   Fare        1308 non-null   float64\n",
      " 9   Cabin       295 non-null    object \n",
      " 10  EMBARKED    1307 non-null   object \n",
      " 11  EMBARKED_S  1309 non-null   bool   \n",
      " 12  EMBARKED_C  1309 non-null   bool   \n",
      " 13  EMBARKED_Q  1309 non-null   bool   \n",
      "dtypes: bool(3), float64(3), int64(3), object(5)\n",
      "memory usage: 116.5+ KB\n"
     ]
    }
   ],
   "source": [
    "if 'pd' not in vars():\n",
    "    import pandas as pd\n",
    "pd.read_csv('data/semi_processed_all.csv').info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing Functions and Script\n",
    "# --------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Declare Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare some global variables\n",
    "using_TSNE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prep environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import open source software packages\n",
    "# numerical manipulation and analysis\n",
    "import numpy as np\n",
    "# data frame manipulation and analysis\n",
    "import pandas as pd\n",
    "# sci-kit learn modules\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#from sklearn.svm import SVC\n",
    "#from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "#from sklearn.gaussian_process.kernels import RBF\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "#from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "# import builtins\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "if using_TSNE:\n",
    "    import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting targets' natural class-weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_class_count_weight_and_recordkeys(targets = [], data = None, verbose = True):\n",
    "    '''\n",
    "    Get the class weights of a set of target columns in a dataframe \n",
    "    (with unique indices)or its equivalent index-oriented dictionary. \n",
    "    Print the information and return it as a dictionary of classes and \n",
    "    weights in the following format:\n",
    "    {\n",
    "        'total_count': tc,\n",
    "        target_col : {\n",
    "            class_a : {\n",
    "                'count': c,\n",
    "                'weight': w,\n",
    "                'records': {i0, i1, ..., 1nc}\n",
    "            },\n",
    "            class_b : {\n",
    "                'count': c,\n",
    "                'weight': w,\n",
    "                'records': {i0, i1, ..., 1nc}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    Expects datetime and pandas to be available.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    targets : list of target-column names in a dataframelike object.\n",
    "        Default []\n",
    "    \n",
    "    data: a pandas.DataFrame or its .to_dict(orient = 'index') equivalent. \n",
    "        If the data is large, you can reduce memory load by simply passing \n",
    "        the target columns. No other information is required.\n",
    "        Default None\n",
    "    \n",
    "    verbose: boolean. Whether or not the function prints out feedback.\n",
    "        Default True\n",
    "    '''\n",
    "    if verbose:\n",
    "        feedback = 'Getting class weights'\n",
    "        print(feedback+'\\n'+'-'*len(feedback))\n",
    "        \n",
    "    # validate non falsy inputs\n",
    "    for var in {'targets': targets, 'data': data}:\n",
    "        # if empty or null\n",
    "        if not var:\n",
    "            raise TypeError(f'''Expected non empty input for {var} but received \"falsy\" type {type(var)}''')\n",
    "    \n",
    "    # if received dataframe cast it to dict and continue\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        # ignore the columns we won't use\n",
    "        data = data[targets].to_dict(orient = 'index')\n",
    "    \n",
    "    # workflow for data dictionary\n",
    "    if isinstance(data, dict):\n",
    "        # initialize storage dict\n",
    "        target_class_weights = {}\n",
    "        \n",
    "        # get length of data\n",
    "        data_length = len(data)\n",
    "        if verbose:\n",
    "            print(datetime.now(), \"Available Data Records:\", f\"{data_length:.2E} = {data_length:,}\\n\")\n",
    "        target_class_weights['total_count'] = data_length\n",
    "        \n",
    "        for target_col in targets:\n",
    "            if verbose:\n",
    "                print(datetime.now(), 'weighing classes in target:', target_col)\n",
    "            # initialize a dict for each target column \n",
    "            target_class_weights[target_col] = {}\n",
    "            \n",
    "            # iterate once through the records to get each classes keys\n",
    "            for record in data:\n",
    "                # get the class label at that record\n",
    "                class_label = data[record][target_col]\n",
    "                \n",
    "                # if first time seeing this class, initialize its own dict\n",
    "                if class_label not in target_class_weights[target_col]:\n",
    "                    # initialize its set of records (to avoid the need for iteration searches downstream)\n",
    "                    target_class_weights[target_col][class_label] = {'records':set()}\n",
    "                \n",
    "                # update this class' set of records\n",
    "                target_class_weights[target_col][class_label]['records'].add(record)\n",
    "            \n",
    "            # now that we have each class's set of records, save its count and weight for convenience\n",
    "            for class_label in target_class_weights[target_col]:\n",
    "                target_class_weights[target_col][class_label]['count'] = len(target_class_weights[target_col][class_label]['records'])\n",
    "                target_class_weights[target_col][class_label]['weight'] = target_class_weights[target_col][class_label]['count']/target_class_weights['total_count']\n",
    "                if verbose:\n",
    "                    print(f\"class:{class_label}\")\n",
    "                    for attribute in ['count', 'weight']:\n",
    "                        print(f\"- {attribute}: {target_class_weights[target_col][class_label][attribute]}\")\n",
    "            if verbose:\n",
    "                print('')\n",
    "    # if neither data frame nor dict\n",
    "    else:\n",
    "        raise TypeError(f'Expected data to be type dict or pd.DataFrame but instead got type: {type(data)}')\n",
    "    \n",
    "    return target_class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting data sample of given weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weighted_data_samples(\n",
    "    class_counts_weights_keys = {}, sample_pct = .10, sample_weights = {}, \n",
    "    verbose = False, return_dict = False\n",
    "):\n",
    "    '''\n",
    "    Takes a dictionary denoting a set of classes, and the data keys that correspond to them in a data set.\n",
    "    Returns a list of random samples to meet the size and weighting specifications.\n",
    "    - Classes will be upsampled or downsampled to meet the requested parameters.\n",
    "    - When upsampling, all unique records will be added once, then additional records will be added at random\n",
    "        with replacement\n",
    "        \n",
    "    Requires numpy.random.choice\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    class_counts_weight_keys: dictionary with the following features where the 'records' contain the \n",
    "        keys or indices of records in a data dictionary or dataframe.\n",
    "        Default {} empty dict (will fail).\n",
    "        {\n",
    "            class_a : {\n",
    "                'count': c,\n",
    "                'weight': w,\n",
    "                'records': {i0, i1, ..., 1c}\n",
    "            },\n",
    "            class_b : {\n",
    "                'count': c,\n",
    "                'weight': w,\n",
    "                'records': {i0, i1, ..., 1c}\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    sample_pct: float between (0,1] denoting the percetage of the original data to be sampled\n",
    "        Default .10\n",
    "        \n",
    "    sample_weights: dictionary of classes and their desired weights in the data sample.\n",
    "        Ex: {class_a:.50, class_b:.50}\n",
    "    \n",
    "    verbose: boolean, whether to print feedback or not\n",
    "        Default False\n",
    "        \n",
    "    return_dict: boolean, whether to return a second object. Optionally Also Returns a similar \n",
    "        dictionary of the same classes denoting their sampled count, sampled weight and a list\n",
    "        of their sampled keys (returns a list because upsampling might require duplicate keys). \n",
    "        Default False\n",
    "        Ex:\n",
    "        {\n",
    "            class_a : {\n",
    "                'count': c,\n",
    "                'weight': w,\n",
    "                'records': [i0, i1, ..., 1c]\n",
    "            },\n",
    "            class_b : {\n",
    "                'count': c,\n",
    "                'weight': w,\n",
    "                'records': [i0, i1, ..., 1c]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    '''\n",
    "    # validate inputs\n",
    "    for variable in [class_counts_weights_keys, sample_weights]:\n",
    "        if not isinstance(variable, dict):\n",
    "            raise TypeError(f\"Expected type dict but received type{type(variable)}.\")\n",
    "    \n",
    "    # initialize list of data keys to return\n",
    "    use_records = []\n",
    "    # initialize dictionary to return\n",
    "    sample_class_counts_weights_keys = {\n",
    "        class_label:{\n",
    "            'count': None,\n",
    "            'weight':None,\n",
    "            'records':[]\n",
    "        } \n",
    "        for class_label in class_counts_weights_keys\n",
    "    }\n",
    "    \n",
    "    # get total count of available data\n",
    "    total_count = 0\n",
    "    for class_label in class_counts_weights_keys:\n",
    "        total_count += class_counts_weights_keys[class_label]['count']\n",
    "        if class_label not in sample_weights:\n",
    "            raise NameError(f\"class: {class_label} not in sample_weights {sample_weights.keys()} If you don't want it in the sample assign its weight to 0\")\n",
    "    \n",
    "    # determine the requested size of the sample\n",
    "    sample_size = int(total_count*sample_pct)\n",
    "    if verbose:\n",
    "        print('Requested sample size:', sample_size)\n",
    "    \n",
    "    # for each class\n",
    "    for class_label in class_counts_weights_keys:\n",
    "        \n",
    "        # determine and save the number of class samples requested\n",
    "        requested_class_count = int(sample_size*sample_weights[class_label])\n",
    "        if verbose:\n",
    "            print('requested_class_count:', class_label, requested_class_count)\n",
    "        \n",
    "        # determine if upsampling will be required\n",
    "        if requested_class_count > class_counts_weights_keys[class_label]['count']:\n",
    "            if verbose:\n",
    "                print(f\"class: {class_label} is too small by {requested_class_count - class_counts_weights_keys[class_label]['count']}\\n Upsampling\")\n",
    "            # add all the unique records available\n",
    "            use_records += list(class_counts_weights_keys[class_label]['records'])\n",
    "            # add all the unique records to the class' sample records\n",
    "            sample_class_counts_weights_keys[class_label]['records'] += list(class_counts_weights_keys[class_label]['records'])\n",
    "            replacement = True\n",
    "            # define how many replacement samples still need to be added\n",
    "            outstanding = requested_class_count - class_counts_weights_keys[class_label]['count']\n",
    "        \n",
    "        # determine if downsampling will be required\n",
    "        if requested_class_count <= class_counts_weights_keys[class_label]['count']:\n",
    "            if verbose:\n",
    "                print(f\"class: {class_label} is too large by {class_counts_weights_keys[class_label]['count'] - requested_class_count}\\n Downsampling\")\n",
    "            replacement = False\n",
    "            outstanding = requested_class_count\n",
    "        \n",
    "        # get outstanding samples to be added\n",
    "        class_samples = np.random.choice(list(class_counts_weights_keys[class_label]['records']), size = outstanding, replace = replacement)\n",
    "        class_samples = list(class_samples)\n",
    "        # add the class samples to the list of records to be used.\n",
    "        use_records += class_samples\n",
    "        # add and save the class samples to be used\n",
    "        sample_class_counts_weights_keys[class_label]['records'] += class_samples\n",
    "        \n",
    "        # spot check to make sure it worked as expected\n",
    "        # get the new count of class samples in use_records\n",
    "        returned_class_count = sum([record in class_counts_weights_keys[class_label]['records'] for record in use_records])\n",
    "        returned_class_weight = returned_class_count/sample_size\n",
    "        assert returned_class_count == requested_class_count\n",
    "        # save class sample counts and weights for output\n",
    "        sample_class_counts_weights_keys[class_label]['count'] = returned_class_count\n",
    "        sample_class_counts_weights_keys[class_label]['weight'] = returned_class_weight\n",
    "        if verbose:\n",
    "            print(f\"class:{class_label}\\n resampled to weight:{np.round(returned_class_weight, 2)}, count:{returned_class_count}\\n\")\n",
    "    \n",
    "    if verbose:\n",
    "            print(datetime.now(), 'Done resampling')\n",
    "            \n",
    "    if not return_dict:\n",
    "        return use_records\n",
    "    else:\n",
    "        return use_records, sample_class_counts_weights_keys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare \"Script\" Variables (i.e. input parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare path to data\n",
    "path_to_data = 'data/semi_processed_all.csv'\n",
    "# declare target columns\n",
    "target_columns = ['EMBARKED_Q', 'EMBARKED_S', 'EMBARKED_C']\n",
    "# declare columns to drop, if any. Requied to be at least an empty list\n",
    "drop_columns = ['EMBARKED']\n",
    "# declare date columns\n",
    "date_cols = []\n",
    "# declare numeric cols\n",
    "numeric_cols = ['Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n",
    "# declare categorical cols\n",
    "categorical_cols = ['Name','Sex','Ticket', 'Cabin']\n",
    "\n",
    "verbose = False\n",
    "\n",
    "# declare sample sizes as percentages of total data size\n",
    "sample_size_percents = [.10, .20, .40, .80, 1.00]\n",
    "# validate sample pcts\n",
    "for pct in sample_size_percents:\n",
    "    if (pct <= 0) or (pct > 1):\n",
    "        raise ValueError(f\"Expected percentage between (0, 1] but received {pct}\")\n",
    "\n",
    "# declare weight configurations to use\n",
    "use_natural_weights = True\n",
    "use_balanced_weights = True\n",
    "class_weight_configs = [{True:.20, False:.80},{True:.30, False:.70}]\n",
    "# Validation for each class_weight configuration\n",
    "for weighting in class_weight_configs:\n",
    "    # spot check that they add up to 1 (for 100%)\n",
    "    if not np.round(pd.Series(weighting).sum(), 2) == 1:\n",
    "        raise ValueError(f'Invalid or incomplete class weight values. Expected sum to about 1 but got {pd.Series(weighting)}')\n",
    "\n",
    "# declare classifiers to use\n",
    "classifiers = {\n",
    "    \"Complement_Naive_Bayes\" : ComplementNB(),\n",
    "    \"Random_Forest_Classifier\": RandomForestClassifier(),\n",
    "    \"Nearest_Neighbors\" : KNeighborsClassifier(),\n",
    "    \"Add_a_Boost\": AdaBoostClassifier(),\n",
    "    \"Neural_Network\" : MLPClassifier()\n",
    "}\n",
    "# declare which models need their features scaled\n",
    "needs_scaling = {\"Nearest_Neighbors\", \"Neural_Network\"}\n",
    "# declare which models need non-negative features\n",
    "needs_non_negative = {'Complement_Naive_Bayes'}\n",
    "\n",
    "        \n",
    "# if testing/debuging on a single target\n",
    "one_run = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting class weights\n",
      "---------------------\n",
      "2020-04-06 10:19:20.191313 Available Data Records: 1.31E+03 = 1,309\n",
      "\n",
      "2020-04-06 10:19:20.191363 weighing classes in target: EMBARKED_Q\n",
      "class:False\n",
      "- count: 1186\n",
      "- weight: 0.906035141329259\n",
      "class:True\n",
      "- count: 123\n",
      "- weight: 0.09396485867074103\n",
      "\n",
      "2020-04-06 10:19:20.191778 weighing classes in target: EMBARKED_S\n",
      "class:True\n",
      "- count: 914\n",
      "- weight: 0.6982429335370511\n",
      "class:False\n",
      "- count: 395\n",
      "- weight: 0.3017570664629488\n",
      "\n",
      "2020-04-06 10:19:20.192524 weighing classes in target: EMBARKED_C\n",
      "class:False\n",
      "- count: 1039\n",
      "- weight: 0.7937356760886173\n",
      "class:True\n",
      "- count: 270\n",
      "- weight: 0.20626432391138275\n",
      "\n",
      "2020-04-06 10:19:20.194966 WORKING ON TARGET COLUMN: EMBARKED_Q\n",
      "-------------------------------------------------\n",
      "2020-04-06 10:19:20.195416 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.1, 'training_weights': {False: 0.91, True: 0.09}, 'model': 'Complement_Naive_Bayes', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.6842105263157895\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[25 10]\n",
      " [ 2  1]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:20.302046 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.1, 'training_weights': {False: 0.91, True: 0.09}, 'model': 'Random_Forest_Classifier', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.9210526315789473\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[35  0]\n",
      " [ 3  0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:20.431693 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.1, 'training_weights': {False: 0.91, True: 0.09}, 'model': 'Nearest_Neighbors', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "----------------------------\n",
      "ACCURACY: 0.9210526315789473\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[35  0]\n",
      " [ 3  0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:20.442018 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.1, 'training_weights': {False: 0.91, True: 0.09}, 'model': 'Add_a_Boost', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "----------------------------\n",
      "ACCURACY: 0.9473684210526315\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[35  0]\n",
      " [ 2  1]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:20.566358 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.1, 'training_weights': {False: 0.91, True: 0.09}, 'model': 'Neural_Network', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
      "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "              warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.9210526315789473\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[35  0]\n",
      " [ 3  0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:20.792835 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.1, 'training_weights': {True: 0.2, False: 0.8}, 'model': 'Complement_Naive_Bayes', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.47368421052631576\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[15 20]\n",
      " [ 0  3]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:20.881924 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.1, 'training_weights': {True: 0.2, False: 0.8}, 'model': 'Random_Forest_Classifier', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 1.0\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[35  0]\n",
      " [ 0  3]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:21.008514 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.1, 'training_weights': {True: 0.2, False: 0.8}, 'model': 'Nearest_Neighbors', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "----------------------------\n",
      "ACCURACY: 0.9210526315789473\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[35  0]\n",
      " [ 3  0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:21.021024 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.1, 'training_weights': {True: 0.2, False: 0.8}, 'model': 'Add_a_Boost', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "----------------------------\n",
      "ACCURACY: 0.34210526315789475\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[10 25]\n",
      " [ 0  3]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:21.087545 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.1, 'training_weights': {True: 0.2, False: 0.8}, 'model': 'Neural_Network', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
      "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "              warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.42105263157894735\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[14 21]\n",
      " [ 1  2]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:21.284276 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.1, 'training_weights': {True: 0.3, False: 0.7}, 'model': 'Complement_Naive_Bayes', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.5\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[16 19]\n",
      " [ 0  3]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:21.458041 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.1, 'training_weights': {True: 0.3, False: 0.7}, 'model': 'Random_Forest_Classifier', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.9210526315789473\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[35  0]\n",
      " [ 3  0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:21.670270 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.1, 'training_weights': {True: 0.3, False: 0.7}, 'model': 'Nearest_Neighbors', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "----------------------------\n",
      "ACCURACY: 0.8157894736842105\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[31  4]\n",
      " [ 3  0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:21.678629 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.1, 'training_weights': {True: 0.3, False: 0.7}, 'model': 'Add_a_Boost', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "----------------------------\n",
      "ACCURACY: 0.9473684210526315\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[33  2]\n",
      " [ 0  3]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:21.746118 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.1, 'training_weights': {True: 0.3, False: 0.7}, 'model': 'Neural_Network', 'tested': False} \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFIER:\n",
      " MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
      "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "              warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.8947368421052632\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[34  1]\n",
      " [ 3  0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:21.880617 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.1, 'training_weights': {False: 0.5, True: 0.5}, 'model': 'Complement_Naive_Bayes', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.6052631578947368\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[20 15]\n",
      " [ 0  3]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:21.968103 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.1, 'training_weights': {False: 0.5, True: 0.5}, 'model': 'Random_Forest_Classifier', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.9210526315789473\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[35  0]\n",
      " [ 3  0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:22.093663 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.1, 'training_weights': {False: 0.5, True: 0.5}, 'model': 'Nearest_Neighbors', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "----------------------------\n",
      "ACCURACY: 0.6842105263157895\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[25 10]\n",
      " [ 2  1]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:22.106827 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.1, 'training_weights': {False: 0.5, True: 0.5}, 'model': 'Add_a_Boost', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "----------------------------\n",
      "ACCURACY: 0.9210526315789473\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[34  1]\n",
      " [ 2  1]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:22.181309 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.1, 'training_weights': {False: 0.5, True: 0.5}, 'model': 'Neural_Network', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
      "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "              warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.9210526315789473\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[35  0]\n",
      " [ 3  0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:22.294510 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.2, 'training_weights': {False: 0.91, True: 0.09}, 'model': 'Complement_Naive_Bayes', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.6493506493506493\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[44 26]\n",
      " [ 1  6]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:22.431072 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.2, 'training_weights': {False: 0.91, True: 0.09}, 'model': 'Random_Forest_Classifier', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.9090909090909091\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[70  0]\n",
      " [ 7  0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:22.563123 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.2, 'training_weights': {False: 0.91, True: 0.09}, 'model': 'Nearest_Neighbors', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "----------------------------\n",
      "ACCURACY: 0.9090909090909091\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[70  0]\n",
      " [ 7  0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:22.576580 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.2, 'training_weights': {False: 0.91, True: 0.09}, 'model': 'Add_a_Boost', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "----------------------------\n",
      "ACCURACY: 0.8961038961038961\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[69  1]\n",
      " [ 7  0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:22.649802 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.2, 'training_weights': {False: 0.91, True: 0.09}, 'model': 'Neural_Network', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
      "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "              warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.8831168831168831\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[68  2]\n",
      " [ 7  0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:22.849910 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.2, 'training_weights': {True: 0.2, False: 0.8}, 'model': 'Complement_Naive_Bayes', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.45454545454545453\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[29 41]\n",
      " [ 1  6]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:22.965275 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.2, 'training_weights': {True: 0.2, False: 0.8}, 'model': 'Random_Forest_Classifier', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.9090909090909091\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[70  0]\n",
      " [ 7  0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:23.100774 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.2, 'training_weights': {True: 0.2, False: 0.8}, 'model': 'Nearest_Neighbors', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "----------------------------\n",
      "ACCURACY: 0.8571428571428571\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[66  4]\n",
      " [ 7  0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:23.116123 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.2, 'training_weights': {True: 0.2, False: 0.8}, 'model': 'Add_a_Boost', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "----------------------------\n",
      "ACCURACY: 0.9090909090909091\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[70  0]\n",
      " [ 7  0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:23.188683 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.2, 'training_weights': {True: 0.2, False: 0.8}, 'model': 'Neural_Network', 'tested': False} \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFIER:\n",
      " MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
      "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "              warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.8441558441558441\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[65  5]\n",
      " [ 7  0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:23.401495 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.2, 'training_weights': {True: 0.3, False: 0.7}, 'model': 'Complement_Naive_Bayes', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.5194805194805194\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[34 36]\n",
      " [ 1  6]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:23.623604 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.2, 'training_weights': {True: 0.3, False: 0.7}, 'model': 'Random_Forest_Classifier', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.9090909090909091\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[70  0]\n",
      " [ 7  0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:23.869836 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.2, 'training_weights': {True: 0.3, False: 0.7}, 'model': 'Nearest_Neighbors', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "----------------------------\n",
      "ACCURACY: 0.6623376623376623\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[51 19]\n",
      " [ 7  0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:23.897493 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.2, 'training_weights': {True: 0.3, False: 0.7}, 'model': 'Add_a_Boost', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "----------------------------\n",
      "ACCURACY: 0.8961038961038961\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[69  1]\n",
      " [ 7  0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:23.981446 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.2, 'training_weights': {True: 0.3, False: 0.7}, 'model': 'Neural_Network', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
      "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "              warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.9090909090909091\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[70  0]\n",
      " [ 7  0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:24.242248 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.2, 'training_weights': {False: 0.5, True: 0.5}, 'model': 'Complement_Naive_Bayes', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.37662337662337664\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[24 46]\n",
      " [ 2  5]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:24.353014 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.2, 'training_weights': {False: 0.5, True: 0.5}, 'model': 'Random_Forest_Classifier', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.9090909090909091\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[70  0]\n",
      " [ 7  0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:24.502430 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.2, 'training_weights': {False: 0.5, True: 0.5}, 'model': 'Nearest_Neighbors', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "----------------------------\n",
      "ACCURACY: 0.09090909090909091\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[ 0 70]\n",
      " [ 0  7]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:24.518007 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.2, 'training_weights': {False: 0.5, True: 0.5}, 'model': 'Add_a_Boost', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "----------------------------\n",
      "ACCURACY: 0.8181818181818182\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[59 11]\n",
      " [ 3  4]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:24.608185 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.2, 'training_weights': {False: 0.5, True: 0.5}, 'model': 'Neural_Network', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
      "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "              warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.2857142857142857\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[18 52]\n",
      " [ 3  4]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:24.815224 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.4, 'training_weights': {False: 0.91, True: 0.09}, 'model': 'Complement_Naive_Bayes', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.6794871794871795\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[96 46]\n",
      " [ 4 10]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:24.982054 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.4, 'training_weights': {False: 0.91, True: 0.09}, 'model': 'Random_Forest_Classifier', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.9102564102564102\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[142   0]\n",
      " [ 14   0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:25.133394 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.4, 'training_weights': {False: 0.91, True: 0.09}, 'model': 'Nearest_Neighbors', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "----------------------------\n",
      "ACCURACY: 0.782051282051282\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[122  20]\n",
      " [ 14   0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:25.189954 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.4, 'training_weights': {False: 0.91, True: 0.09}, 'model': 'Add_a_Boost', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "----------------------------\n",
      "ACCURACY: 0.9166666666666666\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[134   8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [  5   9]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:25.331957 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.4, 'training_weights': {False: 0.91, True: 0.09}, 'model': 'Neural_Network', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
      "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "              warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.7948717948717948\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[124  18]\n",
      " [ 14   0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:25.668810 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.4, 'training_weights': {True: 0.2, False: 0.8}, 'model': 'Complement_Naive_Bayes', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.5833333333333334\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[77 65]\n",
      " [ 0 14]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:25.867982 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.4, 'training_weights': {True: 0.2, False: 0.8}, 'model': 'Random_Forest_Classifier', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.9102564102564102\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[142   0]\n",
      " [ 14   0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:26.036915 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.4, 'training_weights': {True: 0.2, False: 0.8}, 'model': 'Nearest_Neighbors', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "----------------------------\n",
      "ACCURACY: 0.717948717948718\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[112  30]\n",
      " [ 14   0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:26.078050 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.4, 'training_weights': {True: 0.2, False: 0.8}, 'model': 'Add_a_Boost', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "----------------------------\n",
      "ACCURACY: 0.9230769230769231\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[137   5]\n",
      " [  7   7]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:26.191509 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.4, 'training_weights': {True: 0.2, False: 0.8}, 'model': 'Neural_Network', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
      "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "              warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.5192307692307693\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[75 67]\n",
      " [ 8  6]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:26.618779 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.4, 'training_weights': {True: 0.3, False: 0.7}, 'model': 'Complement_Naive_Bayes', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.48717948717948717\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[66 76]\n",
      " [ 4 10]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:26.771083 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.4, 'training_weights': {True: 0.3, False: 0.7}, 'model': 'Random_Forest_Classifier', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.9102564102564102\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[142   0]\n",
      " [ 14   0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:26.968662 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.4, 'training_weights': {True: 0.3, False: 0.7}, 'model': 'Nearest_Neighbors', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "----------------------------\n",
      "ACCURACY: 0.09615384615384616\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[  1 141]\n",
      " [  0  14]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:27.019928 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.4, 'training_weights': {True: 0.3, False: 0.7}, 'model': 'Add_a_Boost', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "----------------------------\n",
      "ACCURACY: 0.21794871794871795\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[ 20 122]\n",
      " [  0  14]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:27.130445 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.4, 'training_weights': {True: 0.3, False: 0.7}, 'model': 'Neural_Network', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
      "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "              warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.20512820512820512\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[ 19 123]\n",
      " [  1  13]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:27.511217 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.4, 'training_weights': {False: 0.5, True: 0.5}, 'model': 'Complement_Naive_Bayes', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.44871794871794873\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[57 85]\n",
      " [ 1 13]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:27.715085 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.4, 'training_weights': {False: 0.5, True: 0.5}, 'model': 'Random_Forest_Classifier', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.9038461538461539\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[141   1]\n",
      " [ 14   0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:27.949918 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.4, 'training_weights': {False: 0.5, True: 0.5}, 'model': 'Nearest_Neighbors', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "----------------------------\n",
      "ACCURACY: 0.09615384615384616\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[  1 141]\n",
      " [  0  14]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:27.983303 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.4, 'training_weights': {False: 0.5, True: 0.5}, 'model': 'Add_a_Boost', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "----------------------------\n",
      "ACCURACY: 0.9423076923076923\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[139   3]\n",
      " [  6   8]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:28.072033 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.4, 'training_weights': {False: 0.5, True: 0.5}, 'model': 'Neural_Network', 'tested': False} \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFIER:\n",
      " MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
      "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "              warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.1858974358974359\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[ 15 127]\n",
      " [  0  14]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:28.412213 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.8, 'training_weights': {False: 0.91, True: 0.09}, 'model': 'Complement_Naive_Bayes', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.6389776357827476\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[176 109]\n",
      " [  4  24]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:28.711128 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.8, 'training_weights': {False: 0.91, True: 0.09}, 'model': 'Random_Forest_Classifier', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.9105431309904153\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[285   0]\n",
      " [ 28   0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:28.892628 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.8, 'training_weights': {False: 0.91, True: 0.09}, 'model': 'Nearest_Neighbors', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "----------------------------\n",
      "ACCURACY: 0.9073482428115016\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[284   1]\n",
      " [ 28   0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:29.072571 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.8, 'training_weights': {False: 0.91, True: 0.09}, 'model': 'Add_a_Boost', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "----------------------------\n",
      "ACCURACY: 0.9105431309904153\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[285   0]\n",
      " [ 28   0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:29.240547 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.8, 'training_weights': {False: 0.91, True: 0.09}, 'model': 'Neural_Network', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
      "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "              warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.5527156549520766\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[155 130]\n",
      " [ 10  18]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:29.780925 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.8, 'training_weights': {True: 0.2, False: 0.8}, 'model': 'Complement_Naive_Bayes', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.4952076677316294\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[129 156]\n",
      " [  2  26]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:30.164189 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.8, 'training_weights': {True: 0.2, False: 0.8}, 'model': 'Random_Forest_Classifier', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.9105431309904153\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[285   0]\n",
      " [ 28   0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:30.496231 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.8, 'training_weights': {True: 0.2, False: 0.8}, 'model': 'Nearest_Neighbors', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "----------------------------\n",
      "ACCURACY: 0.43450479233226835\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[116 169]\n",
      " [  8  20]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:30.671667 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.8, 'training_weights': {True: 0.2, False: 0.8}, 'model': 'Add_a_Boost', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "----------------------------\n",
      "ACCURACY: 0.9488817891373802\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[281   4]\n",
      " [ 12  16]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:30.849468 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.8, 'training_weights': {True: 0.2, False: 0.8}, 'model': 'Neural_Network', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
      "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "              warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.7859424920127795\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[241  44]\n",
      " [ 23   5]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:32.317110 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.8, 'training_weights': {True: 0.3, False: 0.7}, 'model': 'Complement_Naive_Bayes', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.4472843450479233\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[115 170]\n",
      " [  3  25]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:32.604640 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.8, 'training_weights': {True: 0.3, False: 0.7}, 'model': 'Random_Forest_Classifier', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.9073482428115016\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[283   2]\n",
      " [ 27   1]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:32.813160 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.8, 'training_weights': {True: 0.3, False: 0.7}, 'model': 'Nearest_Neighbors', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "----------------------------\n",
      "ACCURACY: 0.3546325878594249\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[ 93 192]\n",
      " [ 10  18]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:33.002445 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.8, 'training_weights': {True: 0.3, False: 0.7}, 'model': 'Add_a_Boost', 'tested': False} \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFIER:\n",
      " AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "----------------------------\n",
      "ACCURACY: 0.33226837060702874\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[ 77 208]\n",
      " [  1  27]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:33.161918 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.8, 'training_weights': {True: 0.3, False: 0.7}, 'model': 'Neural_Network', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
      "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "              warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.9073482428115016\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[284   1]\n",
      " [ 28   0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:33.791887 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.8, 'training_weights': {False: 0.5, True: 0.5}, 'model': 'Complement_Naive_Bayes', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.5910543130990416\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[164 121]\n",
      " [  7  21]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:34.037764 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.8, 'training_weights': {False: 0.5, True: 0.5}, 'model': 'Random_Forest_Classifier', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.9073482428115016\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[284   1]\n",
      " [ 28   0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:34.298518 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.8, 'training_weights': {False: 0.5, True: 0.5}, 'model': 'Nearest_Neighbors', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "----------------------------\n",
      "ACCURACY: 0.6964856230031949\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[218  67]\n",
      " [ 28   0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:34.465870 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.8, 'training_weights': {False: 0.5, True: 0.5}, 'model': 'Add_a_Boost', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "----------------------------\n",
      "ACCURACY: 0.9361022364217252\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[279   6]\n",
      " [ 14  14]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:34.654938 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 0.8, 'training_weights': {False: 0.5, True: 0.5}, 'model': 'Neural_Network', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
      "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "              warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.9105431309904153\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[285   0]\n",
      " [ 28   0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:35.026732 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 1.0, 'training_weights': {False: 0.91, True: 0.09}, 'model': 'Complement_Naive_Bayes', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.5549872122762148\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[190 166]\n",
      " [  8  27]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:35.353444 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 1.0, 'training_weights': {False: 0.91, True: 0.09}, 'model': 'Random_Forest_Classifier', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.9104859335038363\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[356   0]\n",
      " [ 35   0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:35.664367 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 1.0, 'training_weights': {False: 0.91, True: 0.09}, 'model': 'Nearest_Neighbors', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "----------------------------\n",
      "ACCURACY: 0.9053708439897699\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[354   2]\n",
      " [ 35   0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:35.989178 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 1.0, 'training_weights': {False: 0.91, True: 0.09}, 'model': 'Add_a_Boost', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "----------------------------\n",
      "ACCURACY: 0.9437340153452686\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[353   3]\n",
      " [ 19  16]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:36.187251 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 1.0, 'training_weights': {False: 0.91, True: 0.09}, 'model': 'Neural_Network', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
      "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "              warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.7570332480818415\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[279  77]\n",
      " [ 18  17]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:36.988644 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 1.0, 'training_weights': {True: 0.2, False: 0.8}, 'model': 'Complement_Naive_Bayes', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.5217391304347826\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[173 183]\n",
      " [  4  31]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:37.302965 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 1.0, 'training_weights': {True: 0.2, False: 0.8}, 'model': 'Random_Forest_Classifier', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.9104859335038363\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[356   0]\n",
      " [ 35   0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:37.660134 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 1.0, 'training_weights': {True: 0.2, False: 0.8}, 'model': 'Nearest_Neighbors', 'tested': False} \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFIER:\n",
      " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "----------------------------\n",
      "ACCURACY: 0.21994884910485935\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[ 54 302]\n",
      " [  3  32]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:38.052272 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 1.0, 'training_weights': {True: 0.2, False: 0.8}, 'model': 'Add_a_Boost', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "----------------------------\n",
      "ACCURACY: 0.5882352941176471\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[196 160]\n",
      " [  1  34]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:38.466562 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 1.0, 'training_weights': {True: 0.2, False: 0.8}, 'model': 'Neural_Network', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
      "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "              warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.9002557544757033\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[352   4]\n",
      " [ 35   0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:40.301695 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 1.0, 'training_weights': {True: 0.3, False: 0.7}, 'model': 'Complement_Naive_Bayes', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.5524296675191815\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[190 166]\n",
      " [  9  26]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:40.691254 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 1.0, 'training_weights': {True: 0.3, False: 0.7}, 'model': 'Random_Forest_Classifier', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.9104859335038363\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[356   0]\n",
      " [ 35   0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:41.018191 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 1.0, 'training_weights': {True: 0.3, False: 0.7}, 'model': 'Nearest_Neighbors', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "----------------------------\n",
      "ACCURACY: 0.680306905370844\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[253 103]\n",
      " [ 22  13]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:41.289833 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 1.0, 'training_weights': {True: 0.3, False: 0.7}, 'model': 'Add_a_Boost', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "----------------------------\n",
      "ACCURACY: 0.9002557544757033\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[352   4]\n",
      " [ 35   0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:41.464698 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 1.0, 'training_weights': {True: 0.3, False: 0.7}, 'model': 'Neural_Network', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
      "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "              warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.9104859335038363\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[356   0]\n",
      " [ 35   0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:43.060434 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 1.0, 'training_weights': {False: 0.5, True: 0.5}, 'model': 'Complement_Naive_Bayes', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.5166240409207161\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[173 183]\n",
      " [  6  29]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:43.268606 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 1.0, 'training_weights': {False: 0.5, True: 0.5}, 'model': 'Random_Forest_Classifier', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.9104859335038363\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[356   0]\n",
      " [ 35   0]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:43.469872 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 1.0, 'training_weights': {False: 0.5, True: 0.5}, 'model': 'Nearest_Neighbors', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "----------------------------\n",
      "ACCURACY: 0.1969309462915601\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[ 45 311]\n",
      " [  3  32]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:43.641962 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 1.0, 'training_weights': {False: 0.5, True: 0.5}, 'model': 'Add_a_Boost', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "----------------------------\n",
      "ACCURACY: 0.928388746803069\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[345  11]\n",
      " [ 17  18]] \n",
      "\n",
      "\n",
      "\n",
      "2020-04-06 10:19:43.880335 Working on task:\n",
      "-------------------------------------------\n",
      "{'pct': 1.0, 'training_weights': {False: 0.5, True: 0.5}, 'model': 'Neural_Network', 'tested': False} \n",
      "\n",
      "CLASSIFIER:\n",
      " MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
      "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "              warm_start=False)\n",
      "----------------------------\n",
      "ACCURACY: 0.9104859335038363\n",
      "----------------------------\n",
      "Confusion Matrix \n",
      "[[356   0]\n",
      " [ 35   0]] \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if verbose:\n",
    "    feedback = f\"{datetime.now()} GETTING TARGET-DATA\" + '\\n'\n",
    "    feedback = feedback + '='*len(feedback)\n",
    "    print(feedback)\n",
    "# get target data\n",
    "if path_to_data.split('.')[-1].lower() == 'csv':\n",
    "    # if data frame read only target columns then cast to dict\n",
    "    target_data = pd.read_csv(path_to_data, usecols=target_columns).to_dict(orient = 'index')\n",
    "if path_to_data.split('.')[-1].lower() == 'json':\n",
    "    # if json read whole thing\n",
    "    with open(path_to_data, 'r') as json_file:\n",
    "        target_data = json.loads(json_file.read())\n",
    "        # then reduce to target features\n",
    "        target_data = {record:{target:target_data[record][target] for target in target_columns} for record in target_data}\n",
    "        \n",
    "# get targets' class counts, weights, and record keys\n",
    "if verbose:\n",
    "    print(datetime.now(), 'getting targets_class_counts_weights_recordKeys...')\n",
    "targets_class_counts_weights_recordKeys = get_target_class_count_weight_and_recordkeys(\n",
    "    targets = target_columns, \n",
    "    data = target_data,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# START WORKING ON TARGET COLUMNS\n",
    "if verbose:\n",
    "    print(datetime.now(), 'STARTING WORK FOR TARGET COLUMNS:')\n",
    "    print('==============================================================')\n",
    "    print(target_columns)\n",
    "# for each target column in a single data frame\n",
    "for target in target_columns:\n",
    "    print(datetime.now(), 'WORKING ON TARGET COLUMN:', target)\n",
    "    print('-------------------------------------------------')\n",
    "    \n",
    "    # get all available data record keys\n",
    "    all_data_record_keys = set(target_data.keys())\n",
    "    \n",
    "    # get target's natural classes, counts, weights, and recordkeys\n",
    "    natural_class_counts_weights_keys = targets_class_counts_weights_recordKeys[target]\n",
    "    \n",
    "    # finalize list of class_weight_configs to use\n",
    "    if use_natural_weights:\n",
    "        natural_weights = {\n",
    "            class_label:np.round(natural_class_counts_weights_keys[class_label]['weight'], 2) \n",
    "            for class_label in natural_class_counts_weights_keys\n",
    "        }\n",
    "        # make this the first item in the list\n",
    "        if natural_weights not in class_weight_configs:\n",
    "            class_weight_configs = [natural_weights] + class_weight_configs\n",
    "    # get balanced class weights\n",
    "    if use_balanced_weights:\n",
    "        balanced_weights = {\n",
    "            clss:np.round(1/len(class_weight_configs[0]),2) for clss in class_weight_configs[0]\n",
    "        }\n",
    "        # make this the last item in the list\n",
    "        if balanced_weights not in class_weight_configs:\n",
    "            class_weight_configs += [balanced_weights]\n",
    "        \n",
    "    # define brute force order of test iteration (via embeded dict comprehensions.) Keep this snippet inside the loop because it has to \"pack\" tasks based on the natural and balanced weights which are only derived inside the loop\n",
    "    brute_force_order = [\n",
    "        {'pct':pct, 'training_weights': weighting, 'model': model, 'tested': False}\n",
    "        # pcts are ordered in ascending compute load (ascending size of sample)\n",
    "        for pct in sample_size_percents \n",
    "        # weightings are ordered in descending order of presumed difficulty (natural class imbalance first artifical balance last)\n",
    "        for weighting in class_weight_configs \n",
    "        # technically models are in an unordered dict, but the dict was declared in order of prefered testing\n",
    "        for model in classifiers\n",
    "    ]\n",
    "    if verbose:\n",
    "        print(datetime.now(), 'Defined \"brute force order\" (order of incremental compute load iteration)')\n",
    "        #print('Here are the first 30')\n",
    "        #for i in range(30):\n",
    "        #    print(brute_force_order[i])\n",
    "        #print('\\n')\n",
    "    \n",
    "    # EXECUTE TASKS (each task tests a model on specified data sample size and training-set class-weights)\n",
    "    if verbose:\n",
    "        feedback = \"STARTING WORK ON TASKS\"\n",
    "        feedback = feedback + '\\n' + '='*len(f\"{datetime.now()} {feedback}\")\n",
    "        print(datetime.now(), feedback)\n",
    "    # --------------\n",
    "    # initialize task-state trackers\n",
    "    prior_task_pct = 0.0\n",
    "    prior_task_training_weights = {}\n",
    "    prior_task_model = None\n",
    "\n",
    "    # for each task in brute_force_order\n",
    "    for task in brute_force_order:\n",
    "        \n",
    "        # skip completed tasks\n",
    "        if task['tested']:\n",
    "            print('SKIPPING COMPLETED TASK >>>>')\n",
    "            continue\n",
    "        # give some unsolicited feedback\n",
    "        feedback = f\"{datetime.now()} Working on task:\"\n",
    "        feedback = feedback+'\\n'+'-'*len(feedback)\n",
    "        print(feedback)\n",
    "        print(task, '\\n')\n",
    "        \n",
    "        # check if we need to resample the data (can re-use same training and testing records for all models at this task size and weight)\n",
    "        if (task['pct']!=prior_task_pct) or (task['training_weights']!=prior_task_training_weights):\n",
    "            if verbose:\n",
    "                feedback = str(datetime.now()) + ' Resampling data for task'\n",
    "                feedback += '\\n' + '-'*len(feedback)\n",
    "                print(feedback)\n",
    "            \n",
    "            # DEFINE TEST DATA-SET: \"reserve\" data required for the testing-set as 30% of the sample, \n",
    "            # using natural weights. (If we used manipulated weights for testing we wouldn't be testing \n",
    "            # the model's performance in real-world circumstances, where class imbalance is presumed to be \n",
    "            # sever, rather we would be testing the model's performance on an easier task\n",
    "            # where task imbalance is less severe.)\n",
    "            if verbose:\n",
    "                print(datetime.now(), 'defining test set...')\n",
    "            test_records = get_class_weighted_data_samples(\n",
    "                # resample from all the records available\n",
    "                class_counts_weights_keys = natural_class_counts_weights_keys,\n",
    "                # 30% of the sample (equivalent to 30% of task['pct'] of total records available)\n",
    "                sample_pct = .30*task['pct'],\n",
    "                sample_weights = natural_weights,\n",
    "                verbose = verbose,\n",
    "                return_dict = False\n",
    "            )\n",
    "            # spot-check: a 30% subset of data with the natural balance should never require upsampling\n",
    "            if len(test_records) != len(set(test_records)):\n",
    "                raise ValueError(\n",
    "                    f'Uh oh did not obtain correct number of unique records for test-set expected {len(test_records)} unique records but got {len(set(test_records))}'\n",
    "                )\n",
    "            else:\n",
    "                test_records = set(test_records)\n",
    "            if verbose:\n",
    "                print(datetime.now(), 'done defining test-set!\\n')\n",
    "            \n",
    "            # determine remaining data records available after \"securing\" test set for this task\n",
    "            remaining_records_available_for_training = {record for record in all_data_record_keys if record not in test_records}\n",
    "            # get remaining_record_class_counts_weights_keys\n",
    "            remaining_records_class_counts_weights_keys = get_target_class_count_weight_and_recordkeys(\n",
    "                targets = [target],\n",
    "                # pass only the target feature of each record to avoid having to copy or pass entire remaining data set\n",
    "                data = {record:{target: target_data[record][target]} for record in remaining_records_available_for_training},\n",
    "                verbose = verbose\n",
    "            )[target]\n",
    "            \n",
    "            # DEFINE TRAINING-SET: 70% of the sample\n",
    "            # (To combat overfitting, it's important to do this after the test set is \"secured;\" \n",
    "            # to ensure any upsampling required for completion of the training set does not \n",
    "            # include overlap or \"data leakage\" from the test set)\n",
    "            if verbose:\n",
    "                print(datetime.now(), 'defining training set...')\n",
    "            training_records = get_class_weighted_data_samples(\n",
    "                # resample from the remaining records available after securing test-set\n",
    "                class_counts_weights_keys=remaining_records_class_counts_weights_keys,\n",
    "                # 70% of the sample (equivalent to 70% of task['pct'] of remaining records available)\n",
    "                sample_pct = .70*task['pct'],\n",
    "                sample_weights = task['training_weights'],\n",
    "                verbose = verbose,\n",
    "                return_dict = False\n",
    "            )\n",
    "            if verbose:\n",
    "                print(datetime.now(), 'done defining training-set!', '\\n')\n",
    "            \n",
    "            # update drop_columns by adding the target columns we aren't currently working on\n",
    "            task_drop_columns = drop_columns + [col for col in target_columns if col != target]\n",
    "            \n",
    "            # READ IN TRAINING-DATA FOR TASK\n",
    "            if verbose:\n",
    "                print(datetime.now(), 'READING TRAINING-DATA FOR TASK')\n",
    "                print('------------------------------------------------------------------')\n",
    "            # handle csv data\n",
    "            if path_to_data.split('.')[-1].lower() == 'csv':\n",
    "                # if data frame read and only keep training records (drop unused target columns)\n",
    "                training_data = pd.read_csv(path_to_data).loc[\n",
    "                    # filter to training records\n",
    "                    training_records\n",
    "                ].drop(\n",
    "                    # filter to non-target columns (and drop specified columns if any)\n",
    "                    columns = task_drop_columns\n",
    "                    # reset the index cause it might not be unique after upsampling\n",
    "                ).reset_index(drop = True)\n",
    "                \n",
    "            # hadle json data\n",
    "            if path_to_data.split('.')[-1].lower() == 'json':\n",
    "                # if json read whole thing then reduce to target features\n",
    "                with open(path_to_data, 'r') as json_file:\n",
    "                    training_data = json.loads(json_file.read())\n",
    "                    # filter down to only target records (add an enumerating prefix because there may be duplicate keys due to upsampling)\n",
    "                    training_data = {\n",
    "                        # keep each column and its record value, but ignore (drop specified columns if any)\n",
    "                        f\"{ri}_{record}\":{\n",
    "                            column:training_data[record][column] for column in training_data[record] if column not in task_drop_columns\n",
    "                        }\n",
    "                        for ri, record in enumerate(training_records)\n",
    "                    }\n",
    "                    # cast dict to dataframe for pre-processing\n",
    "                    training_data = pd.DataFrame(training_data).T.reset_index(drop = True)\n",
    "            \n",
    "            # separate training target from features\n",
    "            y_train = training_data[target]\n",
    "            training_data = training_data.drop(columns = [target])\n",
    "            if verbose:\n",
    "                print('done reading training-data!\\n')\n",
    "            \n",
    "            # READ IN TEST-DATA FOR TASK\n",
    "            if verbose:\n",
    "                print(datetime.now(), 'READING TEST-DATA FOR TASK')\n",
    "                print('------------------------------------------------------------------')\n",
    "            # handle csv data\n",
    "            if path_to_data.split('.')[-1].lower() == 'csv':\n",
    "                # if data frame read and only keep training records (drop unused target columns)\n",
    "                testing_data = pd.read_csv(path_to_data).loc[\n",
    "                    # filter to training records\n",
    "                    list(test_records)\n",
    "                ].drop(\n",
    "                    # filter to non-target columns (and drop specified columns if any)\n",
    "                    columns = task_drop_columns\n",
    "                )\n",
    "            # hadle json data\n",
    "            if path_to_data.split('.')[-1].lower() == 'json':\n",
    "                # if json read whole thing then reduce to target features\n",
    "                with open(path_to_data, 'r') as json_file:\n",
    "                    testing_data = json.loads(json_file.read())\n",
    "                    # filter down to only target records\n",
    "                    testing_data = {\n",
    "                        # keep each column and its record value, but ignore (drop specified columns if any)\n",
    "                        record:{\n",
    "                            column:testing_data[record][column] for column in testing_data[record] if column not in task_drop_columns\n",
    "                        }\n",
    "                        for record in test_records\n",
    "                    }\n",
    "                    # cast dict to dataframe for pre-processing\n",
    "                    testing_data = pd.DataFrame(testing_data).T\n",
    "            \n",
    "            # separate testing target from features\n",
    "            y_test = testing_data[target]\n",
    "            testing_data = testing_data.drop(columns = [target])\n",
    "            if verbose:\n",
    "                print('done reading test-data!\\n')\n",
    "            \n",
    "            # PRE-PROCESS DATA SETS\n",
    "            # ---------------------\n",
    "            if verbose:\n",
    "                print(datetime.now(), 'pre-processing data sets...'.upper())\n",
    "                print('----------------------------------------------------')\n",
    "            for data_set in [training_data, testing_data]:\n",
    "                \n",
    "                # process numeric cols\n",
    "                if verbose:\n",
    "                    print('\\n processing numeric columns...')\n",
    "                    print('- - - - - - - - - - - - - - - - -')\n",
    "                for col in numeric_cols:\n",
    "                    # scrub the numeric columns for commas\n",
    "                    dt = data_set.dtypes[col]\n",
    "                    if dt == np.dtype('object'):\n",
    "                        data_set[col] = data_set[col].map(\n",
    "                            lambda s: float(re.sub(',', '', s))\n",
    "                            if s!= None else np.nan\n",
    "                        )\n",
    "                    # ensure they are numeric\n",
    "                    data_set[col] = data_set[col].astype('float64')\n",
    "                    if verbose:\n",
    "                        print(datetime.now(), col, 'casted to float64')\n",
    "                \n",
    "                # process date columns to days ago\n",
    "                today = datetime.today().date()\n",
    "                if verbose:\n",
    "                    print(datetime.now(), '\\n coercing date cols to int64 days ago...')\n",
    "                    print('- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ')\n",
    "                for col in date_cols:\n",
    "                    if np.dtype(data_set[col]) != np.dtype('datetime64[ns]'):\n",
    "                        try:\n",
    "                            data_set[col] = (today - pd.to_datetime(data_set[col], errors='coerce').dt.date).dt.days\n",
    "                            if verbose:\n",
    "                                print(datetime.now(), 'casted', col, 'to', np.dtype(data_set[col]))\n",
    "                        except TypeError:\n",
    "                            # if they are all null, then drop the column\n",
    "                            if not pd.to_datetime(data_set[col], errors='coerce').notna().sum():\n",
    "                                data_set.drop(columns = col, inplace = True)\n",
    "                                print(datetime.now(), 'dropped empty col:', col)\n",
    "                \n",
    "                # IMPUTE MISSING VALUES\n",
    "                if verbose:\n",
    "                    print(datetime.now(), 'IMPUTING MISSING VALUES')\n",
    "                    print('--------------------------------------------')\n",
    "                # declare sentinel categorical to impute with\n",
    "                sentinel_cat = '0'\n",
    "                dry_run = False\n",
    "\n",
    "                for col in data_set.columns:\n",
    "                    # get the data type\n",
    "                    dt = np.dtype(data_set[col])\n",
    "                    if verbose:\n",
    "                        print(datetime.now(), col, ':', dt)\n",
    "\n",
    "                    # handle categoricals\n",
    "                    if dt == np.dtype('object'):\n",
    "                        if verbose:\n",
    "                            print('-found categorical feature')\n",
    "                        # if the nulls sum (n_sm) is > 0\n",
    "                        n_sm = sum(data_set[col].isnull())\n",
    "                        if n_sm > 0:\n",
    "                            if verbose:\n",
    "                                print('Column: ', col, 'has', n_sm, 'nulls')\n",
    "                            if not dry_run:\n",
    "                                data_set[col] = data_set[col].fillna(sentinel_cat)\n",
    "                                if verbose:\n",
    "                                    print('- Imputed')\n",
    "                                    print('- ', col, 'now has', sum(data_set[col].isnull()), 'nulls')\n",
    "\n",
    "                    # for numerical values impute them with the median\n",
    "                    elif dt == np.dtype('float64') or dt == np.dtype('int64'):\n",
    "                        if verbose:\n",
    "                            print('-found numerical feature')\n",
    "                        # if the nulls sum (n_sm) is > 0\n",
    "                        n_sm = sum(data_set[col].isnull())\n",
    "                        if n_sm > 0:\n",
    "                            if verbose:\n",
    "                                print('- Column: ', col, 'has', n_sm, 'nulls')\n",
    "                            if not dry_run:\n",
    "                                md = np.median(data_set[col].dropna())\n",
    "                                data_set[col] = data_set[col].fillna(md)\n",
    "                                if verbose:\n",
    "                                    print('- Imputed')\n",
    "                                    print('- ', col, 'now has', sum(data_set[col].isnull()), 'nulls')\n",
    "                    # for datetime values impute them with the \"manufactured\" median\n",
    "                    elif dt == np.dtype('datetime64[ns]'):\n",
    "                        if verbose:\n",
    "                            print('-found', dt ,'feature')\n",
    "                        # if the nulls sum (n_sm) is > 0\n",
    "                        n_sm = sum(data_set[col].isnull())\n",
    "                        if n_sm > 0:\n",
    "                            if verbose:\n",
    "                                print('- Column: ', col, 'has', n_sm, 'nulls')\n",
    "                            if not dry_run:\n",
    "                                # manufacture the middle value between the max and the min by adding half that distance to the min\n",
    "                                md = data_set[col].min() + (data_set[col].max() - data_set[col].min())/2\n",
    "                                data_set[col] = data_set[col].fillna(md)\n",
    "                                # if there weren't enough values to compute a median and its still null fill it with today's date\n",
    "                                if data_set[col].isna().sum():\n",
    "                                    now = datetime.now()\n",
    "                                    data_set[col] = data_set[col].fillna(now)\n",
    "                                if verbose:\n",
    "                                    print('- Imputed')\n",
    "                                    print('- ', col, 'now has', sum(data_set[col].isnull()), 'nulls')\n",
    "                if verbose:\n",
    "                    print(datetime.now(), 'done imputing!', '\\n')\n",
    "            \n",
    "        # determine if any non-positive columns need to be split for both data sets, (triggerable by any one data set)\n",
    "        # this transformation will be \"model-specific\" and ephemeral to the task's general data but we define it now before \"blowing up\" the number of columns to check by getting dummies\n",
    "        if task['model'] in needs_non_negative:\n",
    "            if verbose:\n",
    "                print(datetime.now(), 'Flagging non-negative columns to split for model:', task['model'])\n",
    "                print('- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ')\n",
    "            # initialize column tracker\n",
    "            split_cols = {}\n",
    "            for data_set in [training_data, testing_data]:\n",
    "                for col in data_set:\n",
    "                    # skip columns already flagged for splitting\n",
    "                    if col in split_cols:\n",
    "                        continue\n",
    "\n",
    "                    # skip categoricals\n",
    "                    if np.dtype(data_set[col]) != np.dtype('object'):\n",
    "                        # check if all the values are greater than 0\n",
    "                        if data_set[col].map(lambda v: v<0).any():\n",
    "                            if verbose:\n",
    "                                print(col, 'is non-positive, will split it')\n",
    "                            # track the col for splitting\n",
    "                            split_cols.add(col)\n",
    "                            # move on to the next column\n",
    "                            continue\n",
    "                            \n",
    "         # check if we need to re-dummy the data (can re-use same training and testing records for all models at this task size and weight)\n",
    "        if (task['pct']!=prior_task_pct) or (task['training_weights']!=prior_task_training_weights):       \n",
    "            # GET DUMMIES\n",
    "            if verbose:\n",
    "                feedback = f\"{datetime.now()} GETTING DUMMIES...\"\n",
    "                feedback = feedback +'\\n'+'-'*len(feedback)\n",
    "                print(feedback)\n",
    "            for data_set in [training_data, testing_data]:\n",
    "                # get dummies for trainning-set\n",
    "                training_data = pd.get_dummies(training_data)\n",
    "                # get dummies for test-set\n",
    "                testing_data = pd.get_dummies(testing_data)\n",
    "                # drop features in test-set not seen in training-set\n",
    "                testing_data = testing_data.drop(columns = [col for col in testing_data.columns if col not in training_data.columns])\n",
    "                # add training-features missing in test-set as empty zero columns\n",
    "                for col in [col for col in training_data.columns if col not in testing_data.columns]:\n",
    "                    testing_data[col] = 0\n",
    "            if verbose:\n",
    "                print(datetime.now(), 'done getting dummies!\\n')\n",
    "            \n",
    "            # DONE WITH GENERAL PREPROCESSING OF DATA AT SAME TASK PCT AND TRAINGING WEIGHT\n",
    "            if verbose:\n",
    "                print(datetime.now(), 'done with general pre-processing of data sets!')\n",
    "                print('-------------------------------------------------------------------')\n",
    "            \n",
    "        # MODEL-SPECIFIC EPHEMERAL PRE-PROCESSING\n",
    "        # ---------------------------------------\n",
    "        if verbose:\n",
    "            print(datetime.now(), 'BEGINNING MODEL-SPECIFIC DATA PRE-PROCESSING')\n",
    "            print('--------------------------------------------------------------------')\n",
    "\n",
    "        # initialize customizable copies of training and test data sets that won't perpetuate their changes to source\n",
    "        X_train = training_data.copy()\n",
    "        X_test = testing_data.copy()\n",
    "\n",
    "        # execute splitting of non-positive columns, if necessary\n",
    "        if task['model'] in needs_non_negative:\n",
    "            # now split the columns in each data-set\n",
    "            for col in split_cols:\n",
    "                for data_set in [X_train, X_test]:\n",
    "                    data_set[f'{col}_positive'] = data_set[col].map(lambda v: v if v >= 0 else 0)\n",
    "                    data_set[f'{col}_negative'] = data_set[col].map(lambda v: abs(v) if v<= 0 else 0)\n",
    "                    data_set.drop(columns = col, inplace = True)\n",
    "            if verbose:\n",
    "                print('For both data-sets, split non-positive columns:', split_cols, '\\n')\n",
    "\n",
    "        # SCALE DATA, IF DESIRED\n",
    "        if task['model'] in needs_scaling:\n",
    "            if verbose:\n",
    "                print(datetime.now(), 'SCALING DATA...')\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "            if verbose:\n",
    "                print(datetime.now(), 'done scaling!\\n')\n",
    "\n",
    "        # FIT MODEL TO TRAININNG-DATA\n",
    "        # instantiate model\n",
    "        clf = classifiers[task['model']]\n",
    "        if verbose:\n",
    "            print(datetime.now(), 'FITTING MODEL TO TRAINING-DATA...')\n",
    "            print('---------------------------------------------------------')\n",
    "        clf.fit(X_train, y_train)\n",
    "        if verbose:\n",
    "            print(datetime.now(), 'done fitting model!')\n",
    "\n",
    "        # TEST PREDICTIONS\n",
    "        if verbose:\n",
    "            print(datetime.now(), 'PREDICTING TEST-DATA...')\n",
    "            print('---------------------------------------------------------')\n",
    "        y_pred = clf.predict(X_test)\n",
    "        if verbose:\n",
    "            print(datetime.now(), 'done predicting!\\n')\n",
    "\n",
    "        # report accuracy\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        print(f\"CLASSIFIER:\\n {clf}\")\n",
    "        print('----------------------------')\n",
    "        print(f'ACCURACY: {acc}')\n",
    "        print('----------------------------')\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(\"Confusion Matrix \")\n",
    "        print(cm, '\\n'*3)\n",
    "\n",
    "\n",
    "        # store previous task's states\n",
    "        prior_task_pct = task['pct']\n",
    "        prior_task_training_weights = task['training_weights']\n",
    "        prior_task_model = task['model']\n",
    "        # note task completion\n",
    "        task['tested'] = True\n",
    "\n",
    "        if verbose:\n",
    "            print(datetime.now(), '>>>>>>>>>>>>DONE WITH TASK!!!\\n')\n",
    "                \n",
    "    # if testing/debugging on a single target\n",
    "    if one_run:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            # MODEL-SPECIFIC EPHEMERAL PRE-PROCESSING\n",
    "            if verbose:\n",
    "                print(datetime.now(), 'BEGINNING MODEL-SPECIFIC DATA PRE-PROCESSING')\n",
    "                print('--------------------------------------------------------------------')\n",
    "            \n",
    "            # initialize customizable copies of training and test data sets that won't perpetuate their changes to source\n",
    "            X_train = training_data.copy()\n",
    "            X_test = testing_data.copy()\n",
    "            \n",
    "            # execute splitting of non-positive columns, if necessary\n",
    "            if task['model'] in needs_non_negative:\n",
    "                # now split the columns in each data-set\n",
    "                for col in split_cols:\n",
    "                    for data_set in [X_train, X_test]:\n",
    "                        data_set[f'{col}_positive'] = data_set[col].map(lambda v: v if v >= 0 else 0)\n",
    "                        data_set[f'{col}_negative'] = data_set[col].map(lambda v: abs(v) if v<= 0 else 0)\n",
    "                        data_set.drop(columns = col, inplace = True)\n",
    "                if verbose:\n",
    "                    print('For both data-sets, split non-positive columns:', split_cols, '\\n')\n",
    "                                    \n",
    "            # SCALE DATA, IF DESIRED\n",
    "            if task['model'] in needs_scaling:\n",
    "                if verbose:\n",
    "                    print(datetime.now(), 'SCALING DATA...')\n",
    "                scaler = StandardScaler()\n",
    "                X_train = scaler.fit_transform(X_train)\n",
    "                X_test = scaler.transform(X_test)\n",
    "                if verbose:\n",
    "                    print(datetime.now(), 'done scaling!\\n')\n",
    "                    \n",
    "            # FIT MODEL TO TRAININNG-DATA\n",
    "            # instantiate model\n",
    "            clf = classifiers[task['model']]\n",
    "            print(type(clf))\n",
    "            if verbose:\n",
    "                print(datetime.now(), 'FITTING MODEL TO TRAINING-DATA...')\n",
    "                print('---------------------------------------------------------')\n",
    "            clf.fit(X_train, y_train)\n",
    "            if verbose:\n",
    "                print(datetime.now(), 'done fitting model!')\n",
    "            \n",
    "            # TEST PREDICTIONS\n",
    "            if verbose:\n",
    "                print(datetime.now(), 'PREDICTING TEST-DATA...')\n",
    "                print('---------------------------------------------------------')\n",
    "            y_pred = clf.predict(X_test)\n",
    "            if verbose:\n",
    "                print(datetime.now(), 'done predicting!')\n",
    "\n",
    "            # report accuracy\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            print(f\"\\nCLASSIFIER:\\n {clf}\")\n",
    "            print('----------------------------')\n",
    "            print(f'ACCURACY: {acc}')\n",
    "            print('----------------------------')\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            print(\"Confusion Matrix \")\n",
    "            print(cm)\n",
    "            \n",
    "            \n",
    "            # store previous task's states\n",
    "            prior_task_pct = task['pct']\n",
    "            prior_task_training_weights = task['training_weights']\n",
    "            prior_task_model = task['model']\n",
    "            # note task completion\n",
    "            task['tested'] = True\n",
    "            \n",
    "            if verbose:\n",
    "                print(datetime.now(), '>>>>>>>>>>>>DONE WITH TASK!!!\\n')\n",
    "                \n",
    "    # if testing/debugging on a single target\n",
    "    if one_run:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "if verbose:\n",
    "    feedback = f\"{datetime.now()} GETTING TARGET-DATA\" + '\\n'\n",
    "    feedback = feedback + '='*len(feedback)\n",
    "    print(feedback)\n",
    "# get target data\n",
    "if path_to_data.split('.')[-1].lower() == 'csv':\n",
    "    # if data frame read only target columns then cast to dict\n",
    "    target_data = pd.read_csv(path_to_data, usecols=target_columns).to_dict(orient = 'index')\n",
    "if path_to_data.split('.')[-1].lower() == 'json':\n",
    "    # if json read whole thing\n",
    "    with open(path_to_data, 'r') as json_file:\n",
    "        target_data = json.loads(json_file.read())\n",
    "        # then reduce to target features\n",
    "        target_data = {record:{target:target_data[record][target] for target in target_columns} for record in target_data}\n",
    "        \n",
    "# get targets' class counts, weights, and record keys\n",
    "if verbose:\n",
    "    print(datetime.now(), 'getting targets_class_counts_weights_recordKeys...')\n",
    "targets_class_counts_weights_recordKeys = get_target_class_count_weight_and_recordkeys(\n",
    "    targets = target_columns, \n",
    "    data = target_data,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# START WORKING ON TARGET COLUMNS\n",
    "if verbose:\n",
    "    print(datetime.now(), 'STARTING WORK FOR TARGET COLUMNS:')\n",
    "    print('==============================================================')\n",
    "    print(target_columns)\n",
    "# for each target column in a single data frame\n",
    "for target in target_columns:\n",
    "    print(datetime.now(), 'WORKING ON TARGET COLUMN:', target)\n",
    "    print('-------------------------------------------------')\n",
    "    \n",
    "    # get all available data record keys\n",
    "    all_data_record_keys = set(target_data.keys())\n",
    "    \n",
    "    # get target's natural classes, counts, weights, and recordkeys\n",
    "    natural_class_counts_weights_keys = targets_class_counts_weights_recordKeys[target]\n",
    "    \n",
    "    # finalize list of class_weight_configs to use\n",
    "    if use_natural_weights:\n",
    "        natural_weights = {\n",
    "            class_label:np.round(natural_class_counts_weights_keys[class_label]['weight'], 2) \n",
    "            for class_label in natural_class_counts_weights_keys\n",
    "        }\n",
    "        # make this the first item in the list\n",
    "        if natural_weights not in class_weight_configs:\n",
    "            class_weight_configs = [natural_weights] + class_weight_configs\n",
    "    # get balanced class weights\n",
    "    if use_balanced_weights:\n",
    "        balanced_weights = {\n",
    "            clss:np.round(1/len(class_weight_configs[0]),2) for clss in class_weight_configs[0]\n",
    "        }\n",
    "        # make this the last item in the list\n",
    "        if balanced_weights not in class_weight_configs:\n",
    "            class_weight_configs += [balanced_weights]\n",
    "        \n",
    "    # define brute force order of test iteration (via embeded dict comprehensions.) Keep this snippet inside the loop because it has to \"pack\" tasks based on the natural and balanced weights which are only derived inside the loop\n",
    "    brute_force_order = [\n",
    "        {'pct':pct, 'training_weights': weighting, 'model': model, 'tested': False}\n",
    "        # pcts are ordered in ascending compute load (ascending size of sample)\n",
    "        for pct in sample_size_percents \n",
    "        # weightings are ordered in descending order of presumed difficulty (natural class imbalance first artifical balance last)\n",
    "        for weighting in class_weight_configs \n",
    "        # technically models are in an unordered dict, but the dict was declared in order of prefered testing\n",
    "        for model in classifiers\n",
    "    ]\n",
    "    if verbose:\n",
    "        print(datetime.now(), 'Defined \"brute force order\" (order of incremental compute load iteration)')\n",
    "        #print('Here are the first 30')\n",
    "        #for i in range(30):\n",
    "        #    print(brute_force_order[i])\n",
    "        #print('\\n')\n",
    "    \n",
    "    # EXECUTE TASKS (each task tests a model on specified data sample size and training-set class-weights)\n",
    "    if verbose:\n",
    "        feedback = \"STARTING WORK ON TASKS\"\n",
    "        feedback = feedback + '\\n' + '='*len(f\"{datetime.now()} {feedback}\")\n",
    "        print(datetime.now(), feedback)\n",
    "    # --------------\n",
    "    # initialize task-state trackers\n",
    "    prior_task_pct = 0.0\n",
    "    prior_task_training_weights = {}\n",
    "    prior_task_model = None\n",
    "\n",
    "    # for each task in brute_force_order\n",
    "    for task in brute_force_order:\n",
    "        # skip completed tasks\n",
    "        if task['tested']:\n",
    "            print('SKIPPING COMPLETED TASK >>>>')\n",
    "            continue\n",
    "        # give some unsolicited feedback\n",
    "        feedback = f\"{datetime.now()} Working on task:\"\n",
    "        feedback = feedback+'\\n'+'-'*len(feedback)\n",
    "        print(feedback)\n",
    "        print(task, '\\n')\n",
    "        # check if we need to resample the data (can re-use same training and testing records for all models at this task size and weight)\n",
    "        if (task['pct']!=prior_task_pct) or (task['training_weights']!=prior_task_training_weights):\n",
    "            if verbose:\n",
    "                feedback = str(datetime.now()) + ' Resampling data for task'\n",
    "                feedback += '\\n' + '-'*len(feedback)\n",
    "                print(feedback)\n",
    "            \n",
    "            # DEFINE TEST DATA-SET: \"reserve\" data required for the testing-set as 30% of the sample, \n",
    "            # using natural weights. (If we used manipulated weights for testing we wouldn't be testing \n",
    "            # the model's performance in real-world circumstances, where class imbalance is presumed to be \n",
    "            # sever, rather we would be testing the model's performance on an easier task\n",
    "            # where task imbalance is less severe.)\n",
    "            if verbose:\n",
    "                print(datetime.now(), 'defining test set...')\n",
    "            test_records = get_class_weighted_data_samples(\n",
    "                # resample from all the records available\n",
    "                class_counts_weights_keys = natural_class_counts_weights_keys,\n",
    "                # 30% of the sample (equivalent to 30% of task['pct'] of total records available)\n",
    "                sample_pct = .30*task['pct'],\n",
    "                sample_weights = natural_weights,\n",
    "                verbose = verbose,\n",
    "                return_dict = False\n",
    "            )\n",
    "            # spot-check: a 30% subset of data with the natural balance should never require upsampling\n",
    "            if len(test_records) != len(set(test_records)):\n",
    "                raise ValueError(\n",
    "                    f'Uh oh did not obtain correct number of unique records for test-set expected {len(test_records)} unique records but got {len(set(test_records))}'\n",
    "                )\n",
    "            else:\n",
    "                test_records = set(test_records)\n",
    "            if verbose:\n",
    "                print(datetime.now(), 'done defining test-set!\\n')\n",
    "            \n",
    "            # determine remaining data records available after \"securing\" test set for this task\n",
    "            remaining_records_available_for_training = {record for record in all_data_record_keys if record not in test_records}\n",
    "            # get remaining_record_class_counts_weights_keys\n",
    "            remaining_records_class_counts_weights_keys = get_target_class_count_weight_and_recordkeys(\n",
    "                targets = [target],\n",
    "                # pass only the target feature of each record to avoid having to copy or pass entire remaining data set\n",
    "                data = {record:{target: target_data[record][target]} for record in remaining_records_available_for_training},\n",
    "                verbose = verbose\n",
    "            )[target]\n",
    "            \n",
    "            # DEFINE TRAINING-SET: 70% of the sample\n",
    "            # (To combat overfitting, it's important to do this after the test set is \"secured;\" \n",
    "            # to ensure any upsampling required for completion of the training set does not \n",
    "            # include overlap or \"data leakage\" from the test set)\n",
    "            if verbose:\n",
    "                print(datetime.now(), 'defining training set...')\n",
    "            training_records = get_class_weighted_data_samples(\n",
    "                # resample from the remaining records available after securing test-set\n",
    "                class_counts_weights_keys=remaining_records_class_counts_weights_keys,\n",
    "                # 70% of the sample (equivalent to 70% of task['pct'] of remaining records available)\n",
    "                sample_pct = .70*task['pct'],\n",
    "                sample_weights = task['training_weights'],\n",
    "                verbose = verbose,\n",
    "                return_dict = False\n",
    "            )\n",
    "            if verbose:\n",
    "                print(datetime.now(), 'done defining training-set!', '\\n')\n",
    "            \n",
    "            # update drop_columns by adding the target columns we aren't currently working on\n",
    "            task_drop_columns = drop_columns + [col for col in target_columns if col != target]\n",
    "            \n",
    "            # READ IN TRAINING-DATA FOR TASK\n",
    "            if verbose:\n",
    "                print(datetime.now(), 'READING TRAINING-DATA FOR TASK')\n",
    "                print('------------------------------------------------------------------')\n",
    "            # handle csv data\n",
    "            if path_to_data.split('.')[-1].lower() == 'csv':\n",
    "                # if data frame read and only keep training records (drop unused target columns)\n",
    "                X_train = pd.read_csv(path_to_data).loc[\n",
    "                    # filter to training records\n",
    "                    training_records\n",
    "                ].drop(\n",
    "                    # filter to non-target columns (and drop specified columns if any)\n",
    "                    columns = task_drop_columns\n",
    "                    # reset the index cause it might not be unique after upsampling\n",
    "                ).reset_index(drop = True)\n",
    "                \n",
    "            # hadle json data\n",
    "            if path_to_data.split('.')[-1].lower() == 'json':\n",
    "                # if json read whole thing then reduce to target features\n",
    "                with open(path_to_data, 'r') as json_file:\n",
    "                    X_train = json.loads(json_file.read())\n",
    "                    # filter down to only target records (add an enumerating prefix because there may be duplicate keys due to upsampling)\n",
    "                    X_train = {\n",
    "                        # keep each column and its record value, but ignore (drop specified columns if any)\n",
    "                        f\"{ri}_{record}\":{\n",
    "                            column:X_train[record][column] for column in X_train[record] if column not in task_drop_columns\n",
    "                        }\n",
    "                        for ri, record in enumerate(training_records)\n",
    "                    }\n",
    "                    # cast dict to dataframe for pre-processing\n",
    "                    X_train = pd.DataFrame(X_train).T.reset_index(drop = True)\n",
    "            \n",
    "            # separate training target from features\n",
    "            y_train = X_train[target]\n",
    "            X_train = X_train.drop(columns = [target])\n",
    "            if verbose:\n",
    "                print('done reading training-data!\\n')\n",
    "            \n",
    "            # READ IN TEST-DATA FOR TASK\n",
    "            if verbose:\n",
    "                print(datetime.now(), 'READING TEST-DATA FOR TASK')\n",
    "                print('------------------------------------------------------------------')\n",
    "            # handle csv data\n",
    "            if path_to_data.split('.')[-1].lower() == 'csv':\n",
    "                # if data frame read and only keep training records (drop unused target columns)\n",
    "                X_test = pd.read_csv(path_to_data).loc[\n",
    "                    # filter to training records\n",
    "                    list(test_records)\n",
    "                ].drop(\n",
    "                    # filter to non-target columns (and drop specified columns if any)\n",
    "                    columns = task_drop_columns\n",
    "                )\n",
    "            # hadle json data\n",
    "            if path_to_data.split('.')[-1].lower() == 'json':\n",
    "                # if json read whole thing then reduce to target features\n",
    "                with open(path_to_data, 'r') as json_file:\n",
    "                    X_test = json.loads(json_file.read())\n",
    "                    # filter down to only target records\n",
    "                    X_test = {\n",
    "                        # keep each column and its record value, but ignore (drop specified columns if any)\n",
    "                        record:{\n",
    "                            column:X_test[record][column] for column in X_test[record] if column not in task_drop_columns\n",
    "                        }\n",
    "                        for record in test_records\n",
    "                    }\n",
    "                    # cast dict to dataframe for pre-processing\n",
    "                    X_test = pd.DataFrame(X_test).T\n",
    "            \n",
    "            # separate testing target from features\n",
    "            y_test = X_test[target]\n",
    "            X_test = X_test.drop(columns = [target])\n",
    "            if verbose:\n",
    "                print('done reading test-data!\\n')\n",
    "            \n",
    "            # PRE-PROCESS DATA SETS\n",
    "            # ---------------------\n",
    "            if verbose:\n",
    "                print(datetime.now(), 'pre-processing data sets...'.upper())\n",
    "                print('----------------------------------------------------')\n",
    "            for data_set in [X_train, X_test]:\n",
    "                \n",
    "                # process numeric cols\n",
    "                if verbose:\n",
    "                    print('\\n processing numeric columns...')\n",
    "                    print('- - - - - - - - - - - - - - - - -')\n",
    "                for col in numeric_cols:\n",
    "                    # scrub the numeric columns for commas\n",
    "                    dt = data_set.dtypes[col]\n",
    "                    if dt == np.dtype('object'):\n",
    "                        data_set[col] = data_set[col].map(\n",
    "                            lambda s: float(re.sub(',', '', s))\n",
    "                            if s!= None else np.nan\n",
    "                        )\n",
    "                    # ensure they are numeric\n",
    "                    data_set[col] = data_set[col].astype('float64')\n",
    "                    if verbose:\n",
    "                        print(datetime.now(), col, 'casted to float64')\n",
    "                \n",
    "                # process date columns to days ago\n",
    "                today = datetime.today().date()\n",
    "                if verbose:\n",
    "                    print(datetime.now(), '\\n coercing date cols to int64 days ago...')\n",
    "                    print('- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ')\n",
    "                for col in date_cols:\n",
    "                    if np.dtype(data_set[col]) != np.dtype('datetime64[ns]'):\n",
    "                        try:\n",
    "                            data_set[col] = (today - pd.to_datetime(data_set[col], errors='coerce').dt.date).dt.days\n",
    "                            if verbose:\n",
    "                                print(datetime.now(), 'casted', col, 'to', np.dtype(data_set[col]))\n",
    "                        except TypeError:\n",
    "                            # if they are all null, then drop the column\n",
    "                            if not pd.to_datetime(data_set[col], errors='coerce').notna().sum():\n",
    "                                data_set.drop(columns = col, inplace = True)\n",
    "                                print(datetime.now(), 'dropped empty col:', col)\n",
    "                \n",
    "                # split non-positive features into two absolute value columns, if model requires it\n",
    "                if task['model'] in needs_non_negative:\n",
    "                    if verbose:\n",
    "                        print(datetime.now(), 'Splitting non-negative columns for model:', task['model'])\n",
    "                        print('- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ')\n",
    "                    for col in data_set:\n",
    "                        # skip categoricals\n",
    "                        if np.dtype(data_set[col]) != np.dtype('object'):\n",
    "                            # check if all the values are greater than 0\n",
    "                            if data_set[col].map(lambda v: v<0).any():\n",
    "                                if verbose:\n",
    "                                    print(col, 'is non-positive, splitting it')\n",
    "                                data_set[f'{col}_positive'] = data_set[col].map(lambda v: v if v >= 0 else 0)\n",
    "                                data_set[f'{col}_negative'] = data_set[col].map(lambda v: abs(v) if v<= 0 else 0)\n",
    "                                data_set.drop(columns = col, inplace = True)\n",
    "                                if verbose:\n",
    "                                    print('Split it!')\n",
    "                \n",
    "                # IMPUTE MISSING VALUES\n",
    "                if verbose:\n",
    "                    print(datetime.now(), 'IMPUTING MISSING VALUES')\n",
    "                    print('--------------------------------------------')\n",
    "                # declare sentinel categorical to impute with\n",
    "                sentinel_cat = '0'\n",
    "                dry_run = False\n",
    "\n",
    "                for col in data_set.columns:\n",
    "                    # get the data type\n",
    "                    dt = np.dtype(data_set[col])\n",
    "                    if verbose:\n",
    "                        print(datetime.now(), col, ':', dt)\n",
    "\n",
    "                    # handle categoricals\n",
    "                    if dt == np.dtype('object'):\n",
    "                        if verbose:\n",
    "                            print('-found categorical feature')\n",
    "                        # if the nulls sum (n_sm) is > 0\n",
    "                        n_sm = sum(data_set[col].isnull())\n",
    "                        if n_sm > 0:\n",
    "                            if verbose:\n",
    "                                print('Column: ', col, 'has', n_sm, 'nulls')\n",
    "                            if not dry_run:\n",
    "                                data_set[col] = data_set[col].fillna(sentinel_cat)\n",
    "                                if verbose:\n",
    "                                    print('- Imputed')\n",
    "                                    print('- ', col, 'now has', sum(data_set[col].isnull()), 'nulls')\n",
    "\n",
    "                    # for numerical values impute them with the median\n",
    "                    elif dt == np.dtype('float64') or dt == np.dtype('int64'):\n",
    "                        if verbose:\n",
    "                            print('-found numerical feature')\n",
    "                        # if the nulls sum (n_sm) is > 0\n",
    "                        n_sm = sum(data_set[col].isnull())\n",
    "                        if n_sm > 0:\n",
    "                            if verbose:\n",
    "                                print('- Column: ', col, 'has', n_sm, 'nulls')\n",
    "                            if not dry_run:\n",
    "                                md = np.median(data_set[col].dropna())\n",
    "                                data_set[col] = data_set[col].fillna(md)\n",
    "                                if verbose:\n",
    "                                    print('- Imputed')\n",
    "                                    print('- ', col, 'now has', sum(data_set[col].isnull()), 'nulls')\n",
    "                    # for datetime values impute them with the \"manufactured\" median\n",
    "                    elif dt == np.dtype('datetime64[ns]'):\n",
    "                        if verbose:\n",
    "                            print('-found', dt ,'feature')\n",
    "                        # if the nulls sum (n_sm) is > 0\n",
    "                        n_sm = sum(data_set[col].isnull())\n",
    "                        if n_sm > 0:\n",
    "                            if verbose:\n",
    "                                print('- Column: ', col, 'has', n_sm, 'nulls')\n",
    "                            if not dry_run:\n",
    "                                # manufacture the middle value between the max and the min by adding half that distance to the min\n",
    "                                md = data_set[col].min() + (data_set[col].max() - data_set[col].min())/2\n",
    "                                data_set[col] = data_set[col].fillna(md)\n",
    "                                # if there weren't enough values to compute a median and its still null fill it with today's date\n",
    "                                if data_set[col].isna().sum():\n",
    "                                    now = datetime.now()\n",
    "                                    data_set[col] = data_set[col].fillna(now)\n",
    "                                if verbose:\n",
    "                                    print('- Imputed')\n",
    "                                    print('- ', col, 'now has', sum(data_set[col].isnull()), 'nulls') \n",
    "                    if verbose:\n",
    "                        print('\\n')\n",
    "            if verbose:\n",
    "                print(datetime.now(), 'done pre-processing data sets!')\n",
    "                print('--------------------------------------------------------')\n",
    "            \n",
    "            # GET DUMMIES\n",
    "            if verbose:\n",
    "                feedback = f\"{datetime.now()} GETTING DUMMIES...\"\n",
    "                feedback = feedback +'\\n'+'-'*len(feedback)\n",
    "                print(feedback)\n",
    "            # get dummies for trainning-set\n",
    "            X_train = pd.get_dummies(X_train)\n",
    "            # get dummies for test-set\n",
    "            X_test = pd.get_dummies(X_test)\n",
    "            # drop features in test-set not seen in training-set\n",
    "            X_test = X_test.drop(columns = [col for col in X_test.columns if col not in X_train.columns])\n",
    "            # add training-features missing in test-set as empty zero columns\n",
    "            for col in [col for col in X_train.columns if col not in X_test.columns]:\n",
    "                X_test[col] = 0\n",
    "            if verbose:\n",
    "                print(datetime.now(), 'done getting dummies!\\n')\n",
    "            \n",
    "            # SCALE DATA, IF DESIRED\n",
    "            if task['model'] in needs_scaling:\n",
    "                if verbose:\n",
    "                    print(datetime.now(), 'SCALING DATA...')\n",
    "                scaler = StandardScaler()\n",
    "                X_train = scaler.fit_transform(X_train)\n",
    "                X_test = scaler.transform(X_test)\n",
    "                if verbose:\n",
    "                    print(datetime.now(), 'done scaling!\\n')\n",
    "                    \n",
    "            # FIT MODEL TO TRAININNG-DATA\n",
    "            # instantiate model\n",
    "            clf = classifiers[task['model']]\n",
    "            print(type(clf))\n",
    "            if verbose:\n",
    "                print(datetime.now(), 'FITTING MODEL TO TRAINING-DATA...')\n",
    "                print('---------------------------------------------------------')\n",
    "            clf.fit(X_train, y_train)\n",
    "            if verbose:\n",
    "                print(datetime.now(), 'done fitting model!')\n",
    "            \n",
    "            # TEST PREDICTIONS\n",
    "            if verbose:\n",
    "                print(datetime.now(), 'PREDICTING TEST-DATA...')\n",
    "                print('---------------------------------------------------------')\n",
    "            y_pred = clf.predict(X_test)\n",
    "            if verbose:\n",
    "                print(datetime.now(), 'done predicting!')\n",
    "\n",
    "            # report accuracy\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            print(f\"\\nCLASSIFIER:\\n {clf}\")\n",
    "            print('----------------------------')\n",
    "            print(f'ACCURACY: {acc}')\n",
    "            print('----------------------------')\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            print(\"Confusion Matrix \")\n",
    "            print(cm)\n",
    "            \n",
    "            \n",
    "            # store previous task's states\n",
    "            prior_task_pct = task['pct']\n",
    "            prior_task_training_weights = task['training_weights']\n",
    "            prior_task_model = task['model']\n",
    "            # note task completion\n",
    "            task['tested'] = True\n",
    "            \n",
    "            if verbose:\n",
    "                print(datetime.now(), '>>>>>>>>>>>>DONE WITH TASK!!!\\n')\n",
    "                \n",
    "    # if testing/debugging on a single target\n",
    "    if one_run:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
